{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q -U bitsandbytes transformers peft accelerate datasets scipy einops evaluate trl rouge_score","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-18T15:10:43.882565Z","iopub.execute_input":"2025-02-18T15:10:43.883093Z","iopub.status.idle":"2025-02-18T15:11:09.020093Z","shell.execute_reply.started":"2025-02-18T15:10:43.883047Z","shell.execute_reply":"2025-02-18T15:11:09.019159Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m99.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m342.1/342.1 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m484.9/484.9 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.6/37.6 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.4/64.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.15.2 which is incompatible.\nmlxtend 0.23.3 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\nplotnine 0.14.4 requires matplotlib>=3.8.0, but you have matplotlib 3.7.5 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport torch\nimport time\nfrom pynvml import *\n\n# Disable Weights & Biases\nos.environ['WANDB_DISABLED'] = \"true\"\n\n# Library Imports\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n    Trainer,\n    GenerationConfig,\n    set_seed\n)\nfrom tqdm import tqdm\nfrom trl import SFTTrainer\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom huggingface_hub import interpreter_login, notebook_login, notebook_login, hf_hub_download\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T15:11:13.345072Z","iopub.execute_input":"2025-02-18T15:11:13.345356Z","iopub.status.idle":"2025-02-18T15:11:35.572613Z","shell.execute_reply.started":"2025-02-18T15:11:13.345331Z","shell.execute_reply":"2025-02-18T15:11:35.571742Z"}},"outputs":[{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# %% [GPU Utilization Check]\ndef print_gpu_utilization():\n    nvmlInit()\n    handle = nvmlDeviceGetHandleByIndex(0)\n    info = nvmlDeviceGetMemoryInfo(handle)\n    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T15:11:35.573840Z","iopub.execute_input":"2025-02-18T15:11:35.574513Z","iopub.status.idle":"2025-02-18T15:11:35.578282Z","shell.execute_reply.started":"2025-02-18T15:11:35.574487Z","shell.execute_reply":"2025-02-18T15:11:35.577453Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# %% [Dataset Loading]\nhuggingface_dataset_name = \"neil-code/dialogsum-test\"\ntry:\n    dataset = load_dataset(huggingface_dataset_name)\nexcept Exception as e:\n    print(f\"Failed to load dataset: {e}\")\n    print(\"Attempting to load dataset from local cache...\")\n    dataset = load_dataset(huggingface_dataset_name, download_mode=\"force_redownload\")\n\n\n# %% [Model & Tokenizer Setup]\nmodel_name = 'microsoft/phi-2'\ncompute_dtype = getattr(torch, \"float16\")\n\n# 4-bit Quantization Config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=False,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T15:11:37.523144Z","iopub.execute_input":"2025-02-18T15:11:37.523426Z","iopub.status.idle":"2025-02-18T15:11:40.943735Z","shell.execute_reply.started":"2025-02-18T15:11:37.523405Z","shell.execute_reply":"2025-02-18T15:11:40.943115Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/4.56k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"341ddd4ae8134e82936d78e0eb77e52e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train.csv:   0%|          | 0.00/1.81M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8886e2c318a843d39a048f8253e81bdf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation.csv:   0%|          | 0.00/441k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7c83ae9f20c46c497d671e9e5aac284"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test.csv:   0%|          | 0.00/447k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9624653b72e7474ea4b5017654731b7f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1999 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e57551fffa5f4fb8b76ec893f90f1c41"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/499 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e698a17edd34241a6a016c1d0c2ee34"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/499 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f893c56102d4c15919f22f0399d921d"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T15:11:55.485602Z","iopub.execute_input":"2025-02-18T15:11:55.485916Z","iopub.status.idle":"2025-02-18T15:12:10.673907Z","shell.execute_reply.started":"2025-02-18T15:11:55.485894Z","shell.execute_reply":"2025-02-18T15:12:10.672994Z"}},"outputs":[{"name":"stdout","text":"\n    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your token (input will not be visible):  ········\nAdd token as git credential? (Y/n)  n\n"}],"execution_count":5},{"cell_type":"code","source":"\n# Load Base Model with Error Handling\ntry:\n    original_model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        device_map=\"auto\",\n        quantization_config=bnb_config,\n        trust_remote_code=True,\n        use_auth_token=True\n    )\nexcept Exception as e:\n    print(f\"Failed to load model from Hugging Face Hub: {e}\")\n    print(\"Attempting to load model from local cache...\")\n    original_model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        device_map=\"auto\",\n        quantization_config=bnb_config,\n        trust_remote_code=True,\n        use_auth_token=True,\n        local_files_only=True  # Force loading from local cache\n    )\n\n# Tokenizer Configuration\ntry:\n    tokenizer = AutoTokenizer.from_pretrained(\n        model_name,\n        trust_remote_code=True,\n        padding_side=\"left\",\n        add_eos_token=True,\n        add_bos_token=True,\n        use_fast=False\n    )\nexcept Exception as e:\n    print(f\"Failed to load tokenizer from Hugging Face Hub: {e}\")\n    print(\"Attempting to load tokenizer from local cache...\")\n    tokenizer = AutoTokenizer.from_pretrained(\n        model_name,\n        trust_remote_code=True,\n        padding_side=\"left\",\n        add_eos_token=True,\n        add_bos_token=True,\n        use_fast=False,\n        local_files_only=True  # Force loading from local cache\n    )\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T15:12:55.826050Z","iopub.execute_input":"2025-02-18T15:12:55.826375Z","iopub.status.idle":"2025-02-18T15:15:16.261622Z","shell.execute_reply.started":"2025-02-18T15:12:55.826348Z","shell.execute_reply":"2025-02-18T15:15:16.260962Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/735 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92d3dc66089e44a887f3ba57ac7ee9ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/35.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f60ccfb62c5c45f5b1dfd30354a0b1db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d9a3184ddfa4f0c8467c54c56001dcf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54213f959274430794d235d74e12d216"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/564M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3449b3658674f79ac2168fa8e6f06d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70f649551ab34cec80b325495873787f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd5814cd25e84b8ca102b17a2d2ea5ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/7.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"723f24a5a7834422a33220a97339f9a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4187f68eb184e489c4f06e9dcee1b7b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba14ba9a541a4e2b87d1a7b6b59c9ebc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/1.08k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88fbf6e042584a5d8a6d56b730d7700e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28d305a30d5046e88fdac482f9febad2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f9b04c3bbbd480b8f34cab1758f008e"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# %% [LoRA Configuration]\npeft_config = LoraConfig(\n    r=64,  # Increased from 32\n    lora_alpha=64,\n    target_modules=[\n        \"Wqkv\",    # Phi-2 specific attention layers\n        \"out_proj\",\n        \"fc1\",\n        \"fc2\",\n    ],\n    bias=\"none\",\n    lora_dropout=0.05,\n    task_type=\"CAUSAL_LM\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T15:15:37.932127Z","iopub.execute_input":"2025-02-18T15:15:37.932624Z","iopub.status.idle":"2025-02-18T15:15:37.937223Z","shell.execute_reply.started":"2025-02-18T15:15:37.932589Z","shell.execute_reply":"2025-02-18T15:15:37.936083Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Prepare Model for Training\noriginal_model = prepare_model_for_kbit_training(original_model)\npeft_model = get_peft_model(original_model, peft_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T15:15:47.307749Z","iopub.execute_input":"2025-02-18T15:15:47.308065Z","iopub.status.idle":"2025-02-18T15:15:48.108979Z","shell.execute_reply.started":"2025-02-18T15:15:47.308042Z","shell.execute_reply":"2025-02-18T15:15:48.108220Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# %% [Training Arguments]\noutput_dir = \"./phi-2-dialogsum-ft\"\ntraining_args = TrainingArguments(\n    output_dir=output_dir,\n    per_device_train_batch_size=2,      # Reduced for memory optimization\n    gradient_accumulation_steps=4,       # Increased for stability\n    warmup_steps=50,\n    max_steps=500,                       # Updated per requirements\n    learning_rate=3e-4,                  # Increased learning rate\n    logging_steps=50,                     # Updated logging interval\n    evaluation_strategy=\"steps\",\n    eval_steps=50,                        # Updated evaluation interval\n    optim=\"paged_adamw_8bit\",\n    save_strategy=\"steps\",\n    save_steps=100,\n    fp16=True,\n    report_to=\"none\",\n    gradient_checkpointing=True,          # Enabled for memory savings\n    group_by_length=True,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T15:15:57.707131Z","iopub.execute_input":"2025-02-18T15:15:57.707465Z","iopub.status.idle":"2025-02-18T15:15:57.740483Z","shell.execute_reply.started":"2025-02-18T15:15:57.707442Z","shell.execute_reply":"2025-02-18T15:15:57.739681Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# %% [Data Preprocessing]\ndef create_prompt_formats(sample):\n    \"\"\"Format training samples with instruction templates\"\"\"\n    INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n    INSTRUCTION_KEY = \"### Instruct: Summarize the below conversation.\"\n    RESPONSE_KEY = \"### Output:\"\n    END_KEY = \"### End\"\n    \n    formatted_prompt = \"\\n\\n\".join([\n        f\"{INTRO_BLURB}\",\n        f\"{INSTRUCTION_KEY}\\n{sample['dialogue']}\",\n        f\"{RESPONSE_KEY}\\n{sample['summary']}\",\n        END_KEY\n    ])\n    sample[\"text\"] = formatted_prompt\n    return sample\n\ndef preprocess_dataset(tokenizer, max_length, dataset):\n    \"\"\"Tokenize and format dataset\"\"\"\n    dataset = dataset.map(create_prompt_formats)\n    processed_dataset = dataset.map(\n        lambda samples: tokenizer(\n            samples[\"text\"],\n            max_length=max_length,\n            truncation=True,\n            padding=\"max_length\"\n        ),\n        batched=True,\n        remove_columns=['id','topic','dialogue','summary','text']\n    )\n    return processed_dataset\n\n# Apply preprocessing\nmax_length = 2048  # Phi-2's context window\ntrain_dataset = preprocess_dataset(tokenizer, max_length, dataset[\"train\"])\neval_dataset = preprocess_dataset(tokenizer, max_length, dataset[\"validation\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T15:16:21.770197Z","iopub.execute_input":"2025-02-18T15:16:21.770529Z","iopub.status.idle":"2025-02-18T15:16:31.773209Z","shell.execute_reply.started":"2025-02-18T15:16:21.770500Z","shell.execute_reply":"2025-02-18T15:16:31.772432Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1999 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac7a575da11848b7bc27bb7600fe6534"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1999 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7072bc7a15ce4cc88db0bfe4f11b52bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/499 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"acd1316e617a497384dcd078cbc769d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/499 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"711027e59be342819a3c4860cc4707de"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"!pip install -U trl transformers datasets peft","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T16:31:09.914264Z","iopub.execute_input":"2025-02-18T16:31:09.914712Z","iopub.status.idle":"2025-02-18T16:31:14.575704Z","shell.execute_reply.started":"2025-02-18T16:31:09.914635Z","shell.execute_reply":"2025-02-18T16:31:14.574765Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: trl in /usr/local/lib/python3.10/dist-packages (0.15.1)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.49.0)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.3.1)\nRequirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.14.0)\nRequirement already satisfied: accelerate>=0.34.0 in /usr/local/lib/python3.10/dist-packages (from trl) (1.4.0)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from trl) (13.9.4)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.28.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.9.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.11)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.5.1+cu121)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (2.19.1)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# %% [Training Setup]\nfrom trl import DataCollatorForCompletionOnlyLM\nfrom trl import SFTTrainer\nfrom transformers import DataCollatorForLanguageModeling\n\n# Initialize the SFTTrainer with latest API\ntrainer = SFTTrainer(\n    model=peft_model,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    peft_config=peft_config,\n    args=training_args,\n    tokenizer=tokenizer,\n    formatting_func=lambda example: [example[\"text\"]],  # Required formatting\n    \n)\n\n# %% [Start Training]\nprint_gpu_utilization()\ntry:\n    trainer.train()\nexcept Exception as e:\n    print(f\"Training failed: {e}\")\n    print(\"Saving model checkpoint before exiting...\")\n    trainer.save_model(output_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T16:31:56.478344Z","iopub.execute_input":"2025-02-18T16:31:56.478737Z","iopub.status.idle":"2025-02-18T21:44:56.229445Z","shell.execute_reply.started":"2025-02-18T16:31:56.478695Z","shell.execute_reply":"2025-02-18T21:44:56.228491Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-23-a9ab56a91e42>:7: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n  trainer = SFTTrainer(\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:382: UserWarning: You passed a dataset that is already processed (contains an `input_ids` field) together with a formatting function. Therefore `formatting_func` will be ignored. Either remove the `formatting_func` or pass a dataset that is not already processed.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Converting train dataset to ChatML:   0%|          | 0/1999 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c116c228b6c24154bf49651b60075da2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Applying chat template to train dataset:   0%|          | 0/1999 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d082d6015f2d43a3bea649736529b700"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Applying chat template to train dataset:   0%|          | 0/1999 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e0e0157b1b64e5abfea7b8207760703"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Converting eval dataset to ChatML:   0%|          | 0/499 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5226fcfe3e149d0a66400ac1b6cca8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Applying chat template to eval dataset:   0%|          | 0/499 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c9a3de07a3a4c97a82af04fd453c10c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Applying chat template to eval dataset:   0%|          | 0/499 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd35e8a7cc734151ae2aa26d452c96e2"}},"metadata":{}},{"name":"stderr","text":"No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"name":"stdout","text":"GPU memory occupied: 3380 MB.\n","output_type":"stream"},{"name":"stderr","text":"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [500/500 5:12:20, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>139.720100</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.000000</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.000000</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>6.908300</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>2.204300</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>170.882300</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.000000</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.048200</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.000000</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.951000</td>\n      <td>nan</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# %% [Model Saving]\ntrainer.model.save_pretrained(output_dir)\ntokenizer.save_pretrained(output_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T21:46:54.648531Z","iopub.execute_input":"2025-02-18T21:46:54.648942Z","iopub.status.idle":"2025-02-18T21:46:55.490516Z","shell.execute_reply.started":"2025-02-18T21:46:54.648909Z","shell.execute_reply":"2025-02-18T21:46:55.489744Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"('./phi-2-dialogsum-ft/tokenizer_config.json',\n './phi-2-dialogsum-ft/special_tokens_map.json',\n './phi-2-dialogsum-ft/vocab.json',\n './phi-2-dialogsum-ft/merges.txt',\n './phi-2-dialogsum-ft/added_tokens.json')"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"# %% [Hugging Face Upload]\ntry:\n    notebook_login()\n    trainer.model.push_to_hub(\"Krati132/phi-2-dialogsum-finetuned\")\n    tokenizer.push_to_hub(\"Krati132/phi-2-dialogsum-finetuned\")\nexcept Exception as e:\n    print(f\"Failed to upload model to Hugging Face Hub: {e}\")\n    print(\"Model saved locally at:\", output_dir)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T21:48:10.569395Z","iopub.execute_input":"2025-02-18T21:48:10.569779Z","iopub.status.idle":"2025-02-18T21:48:19.403987Z","shell.execute_reply.started":"2025-02-18T21:48:10.569748Z","shell.execute_reply":"2025-02-18T21:48:19.403225Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0354fcbb2f284a16b9fd93a5191a1bb7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/210M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"707438c8360f43bc9ba484526408a8ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ac9d50462484ecd97613d17fef82b3e"}},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"# %% [Simplified Evaluation]\ndef generate_summary(model, tokenizer, dialogue, max_new_tokens=50):\n    \"\"\"\n    Generate a summary for a given dialogue.\n    \"\"\"\n    prompt = f\"Instruct: Summarize the following conversation.\\n{dialogue}\\nOutput:\\n\"\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=max_new_tokens,  # Reduced for faster generation\n        temperature=0.3,\n        top_p=0.9,\n        repetition_penalty=1.2,\n    )\n    return tokenizer.decode(outputs[0], skip_special_tokens=True).split(\"Output:\\n\")[-1]\n\n# Ensure the dataset is properly loaded\ntest_samples = dataset[\"test\"]  # Access the test split\nresults = []\n\n# Test on just 2 samples for quick evaluation\nfor i in range(2):  # Use only the first 2 samples\n    sample = test_samples[i]  # Access each sample by index\n    dialogue = sample[\"dialogue\"]\n    summary = sample[\"summary\"]\n\n    # Generate summaries\n    original_output = generate_summary(original_model, tokenizer, dialogue, max_new_tokens=50)\n    finetuned_output = generate_summary(peft_model, tokenizer, dialogue, max_new_tokens=50)\n    \n    # Store results\n    results.append({\n        \"dialogue\": dialogue,\n        \"original_summary\": original_output,\n        \"finetuned_summary\": finetuned_output,\n        \"human_summary\": summary\n    })\n\n# Convert results to DataFrame\nresults_df = pd.DataFrame(results)\nprint(results_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T22:49:20.191777Z","iopub.execute_input":"2025-02-18T22:49:20.192106Z","iopub.status.idle":"2025-02-18T22:49:33.223823Z","shell.execute_reply.started":"2025-02-18T22:49:20.192082Z","shell.execute_reply":"2025-02-18T22:49:33.222917Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"                                            dialogue  \\\n0  #Person1#: Ms. Dawson, I need you to take a di...   \n1  #Person1#: Ms. Dawson, I need you to take a di...   \n\n                                    original_summary  \\\n0  Ms. Dawson instructed her assistant to summari...   \n1  Ms. Dawson instructed her assistant to summari...   \n\n                                   finetuned_summary  \\\n0  Ms. Dawson instructed her assistant to summari...   \n1  Ms. Dawson instructed her assistant to summari...   \n\n                                       human_summary  \n0  Ms. Dawson helps #Person1# to write a memo to ...  \n1  In order to prevent employees from wasting tim...  \n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"import evaluate\n\n# ROUGE Metric Calculation\nrouge = evaluate.load(\"rouge\")\nfinetuned_scores = rouge.compute(\n    predictions=[res[\"finetuned_summary\"] for res in results],\n    references=[res[\"human_summary\"] for res in results]\n)\n\nprint(f\"Fine-tuned Model ROUGE Scores:\")\nprint(f\"ROUGE-1: {finetuned_scores['rouge1']*100:.2f}%\")\nprint(f\"ROUGE-2: {finetuned_scores['rouge2']*100:.2f}%\")\nprint(f\"ROUGE-L: {finetuned_scores['rougeL']*100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T22:50:34.680488Z","iopub.execute_input":"2025-02-18T22:50:34.680891Z","iopub.status.idle":"2025-02-18T22:50:35.983203Z","shell.execute_reply.started":"2025-02-18T22:50:34.680860Z","shell.execute_reply":"2025-02-18T22:50:35.982204Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebc5d790e97d4f61977dd5b514f8b207"}},"metadata":{}},{"name":"stdout","text":"Fine-tuned Model ROUGE Scores:\nROUGE-1: 26.58%\nROUGE-2: 4.18%\nROUGE-L: 18.77%\n","output_type":"stream"}],"execution_count":37},{"cell_type":"markdown","source":"Link to Hugging Face Model: https://huggingface.co/Krati132/phi-2-dialogsum-finetuned","metadata":{}},{"cell_type":"markdown","source":"GitHub Link: https://github.com/kratipandya/Assignment5/tree/main","metadata":{}}]}